{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dfc34ce",
   "metadata": {},
   "source": [
    "## Definition of the analyzed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "1291334f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant:  2\n",
      "Analyzed text: \n",
      "Selling **_Concord_**, a task that might have been easy five years ago, feels\n",
      "like rolling a boulder up a hill in 2024. Developed by new studio Firewalk and\n",
      "published by PlayStation Studios, _Concord_ is a 5v5 hero shooter that riffs on\n",
      "_Guardians of the Galaxy_ with a cassette-era take on a pulpy s...\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import re\n",
    "\n",
    "last_name = \"Крылов\"\n",
    "\n",
    "if not last_name:\n",
    "    raise Exception('Last name is required!')\n",
    "\n",
    "alp = \"абвгдеёжзийклмнопрстуфхцчшщъыьэюя\"\n",
    "w = [1, 4, 21, 25, 34,  6, 44, 26, 13, 44, 38, 26, 4, 43,  4, 49, 46,\n",
    "        17, 42, 29,  4,  9, 36, 34, 31, 22,  15, 30,  4, 19, 28, 28, 33]\n",
    "\n",
    "d = dict(zip(alp, w))\n",
    "variant =  sum([d[el] for el in last_name.lower()]) % 4 + 1\n",
    "\n",
    "print(\"Variant: \", variant)\n",
    "\n",
    "# Construct the file name based on the variant number\n",
    "file_name = f\"data/{variant}.txt\"\n",
    "\n",
    "# Read the contents of the file\n",
    "with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "    TEXT = file.read()\n",
    "\n",
    "text_for_task_3 = TEXT\n",
    "\n",
    "wrapped = textwrap.fill(TEXT, width=80)\n",
    "print(f\"Analyzed text: \\n{wrapped[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f845bf87",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "bf1b2dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed text: \n",
      "selling concord a task that might have been easy five years ago feels like\n",
      "rolling a boulder up a hill in 2024. developed by new studio firewalk and\n",
      "published by playstation studios concord is a 5v5 hero shooter that riffs on\n",
      "guardians of the galaxy with a cassetteera take on a pulpy space setting....\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s.!?0-9\\']', '', text)\n",
    "    text = re.sub(r'([.!?])\\1+', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "preprocessed_text = preprocess_text(TEXT)\n",
    "\n",
    "print(f\"Preprocessed text: \\n{textwrap.fill(preprocessed_text[:300], width=80)}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299657b",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "7d88d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"and\", \"the\", \"is\", \"in\", \"it\", \"you\", \"that\", \"to\", \"of\", \"a\", \"with\", \"for\", \"on\", \"this\", \"at\", \"by\", \"an\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "fab6d2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word frequencies:\n",
      "selling: 1\n",
      "concord: 18\n",
      "task: 1\n",
      "might: 5\n",
      "have: 2\n",
      "been: 1\n",
      "easy: 1\n",
      "five: 1\n",
      "years: 1\n",
      "ago: 1\n"
     ]
    }
   ],
   "source": [
    "def word_frequency(text, stop_words = None):\n",
    "    if (stop_words == None):\n",
    "        stop_words = [\"and\", \"the\", \"is\", \"in\", \"it\", \"you\", \"that\", \"to\", \"of\", \"a\", \"with\", \"for\", \"on\", \"this\", \"at\",\n",
    "                        \"by\", \"an\", \"i\"]\n",
    "    arr = re.sub(r'[^a-z\\s\\']', '', text).split()\n",
    "    d = {}\n",
    "    for word in arr:\n",
    "        if (word not in dict.keys(d) and word not in stop_words):\n",
    "            d[word] = 1\n",
    "        elif (word not in stop_words):\n",
    "            d[word] += 1\n",
    "    return d\n",
    "\n",
    "word_frequencies = word_frequency(preprocessed_text)\n",
    "\n",
    "print(\"Word frequencies:\")\n",
    "for key, value in list(word_frequencies.items())[:10]:\n",
    "        print(key + ': ' + str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b800dbc6",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "631233e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted information: \n",
      "{'emails': [], 'phone_numbers': [], 'dates': [], 'times': [], 'prices': []}\n"
     ]
    }
   ],
   "source": [
    "def extract_information(text, *, email_pattern=r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}',\n",
    "                        phone_pattern=r'[\\+(]\\d[\\d \\-()]{8,12}\\d', date_pattern=r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b',\n",
    "                        time_pattern=r'\\b\\d{1,2}:\\d{2}\\s*[APap][mM]?\\b', price_pattern=r'\\$\\d+(?:\\.\\d{2})?', **custom_patterns):\n",
    "    d = {}\n",
    "    d['emails'] = re.findall(email_pattern, text)\n",
    "    d['phone_numbers'] = re.findall(phone_pattern, text)\n",
    "    d['dates'] = re.findall(date_pattern, text)\n",
    "    d['times'] = re.findall(time_pattern, text)\n",
    "    d['prices'] = re.findall(price_pattern, text)\n",
    "    for key, value in custom_patterns.items():\n",
    "        d[key] = re.findall(value, text)\n",
    "    return d\n",
    "\n",
    "extracted_information = extract_information(text_for_task_3)\n",
    "\n",
    "print(f\"Extracted information: \\n{extracted_information}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90703c7",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "bf5fa6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = [\"good\", \"great\", \"happy\", \"joy\", \"excellent\", \"fantastic\", \"love\", \"best\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "0c3ed6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = [\"bad\", \"sad\", \"hate\", \"terrible\", \"awful\", \"poor\", \"worst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "164b5dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score: \n",
      "9\n"
     ]
    }
   ],
   "source": [
    "def analyze_sentiment(text, positive_words=None, negative_words=None):\n",
    "    if negative_words is None:\n",
    "        negative_words = [\"bad\", \"sad\", \"hate\", \"terrible\", \"awful\", \"poor\", \"worst\"]\n",
    "    if positive_words is None:\n",
    "        positive_words = [\"good\", \"great\", \"happy\", \"joy\", \"excellent\", \"fantastic\", \"love\", \"best\", \"amazing\", \"fun\"]\n",
    "    arr = re.sub(r'[^a-z\\s\\']', '', text).split()\n",
    "    number_of_positive_words = 0\n",
    "    for word in positive_words:\n",
    "        number_of_positive_words += arr.count(word)\n",
    "    number_of_negative_words = 0\n",
    "    for word in negative_words:\n",
    "        number_of_negative_words += arr.count(word)\n",
    "    return number_of_positive_words - number_of_negative_words\n",
    "\n",
    "sentiment_score = analyze_sentiment(preprocessed_text)\n",
    "\n",
    "print(f\"Sentiment score: \\n{sentiment_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64be33d",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "f2e3ef2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text: \n",
      "pros has all the hero shooter staples you'd expect focus on skillsgunplay having\n",
      "some nuance creates fun learning curve gameplay modes are fun cons uninspiring\n",
      "design restrictions on gameplay modes dull enjoyment weird graphics decisions on\n",
      "pc familiar elements with minor twists that aren't earthshattering don't expect\n",
      "anything gamechanging from concord it's difficult to discuss concord without\n",
      "talking about overwatch so it's best to get it out of the way early. final\n",
      "thoughts review score 35 solid or good by screen rant's review metric the\n",
      "upfront cost of concord might make it a hard sell in 2024 but it's certainly\n",
      "nice to play a liveservice shooter that doesn't feel designed to ask for money\n",
      "at every turn.\n"
     ]
    }
   ],
   "source": [
    "def summarize_text(text: str, compression_ratio: float, min_threshold=2):\n",
    "    sentences = re.split(r'(?<=\\w[.!?])\\s+', text)\n",
    "    needed_sentences = max(int(len(sentences) * compression_ratio), min_threshold)\n",
    "\n",
    "    def summarize(sentences):\n",
    "        if len(sentences) <= needed_sentences:\n",
    "            return\n",
    "        level = 100000000000\n",
    "        len_sentence_rang = 3\n",
    "        count_words_rang = 10\n",
    "        sentiment_rang = 100\n",
    "        for sentence in sentences:\n",
    "            sentence_level = len(sentence) * len_sentence_rang + len(sentence.split(' ')) * count_words_rang \\\n",
    "            + abs(analyze_sentiment(sentence)) * sentiment_rang\n",
    "            if (sentence_level < level):\n",
    "                level = sentence_level\n",
    "                min_level_sentence = sentence\n",
    "        sentences.remove(min_level_sentence)\n",
    "        summarize(sentences)\n",
    "    summarize(sentences)\n",
    "    sentences = ' '.join(sentences)\n",
    "    return sentences\n",
    "\n",
    "summarized_text = summarize_text(preprocessed_text, 0.05)\n",
    "\n",
    "print(f\"Summarized text: \\n{textwrap.fill(summarized_text, width=80)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c02d0e",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "37eae2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concord: ******************\n",
      "like: ************\n",
      "it's: **********\n",
      "but: **********\n",
      "one: **********\n",
      "hero: *********\n",
      "more: *********\n",
      "game: ********\n",
      "modes: ********\n",
      "be: ********\n"
     ]
    }
   ],
   "source": [
    "def visualize_word_frequency(dictionary, max_threshold=100):\n",
    "    sorted_d = {key: value for key, value in sorted(dictionary.items(), key=lambda item: item[1], reverse=True)}\n",
    "    temp = 0\n",
    "    if (max_threshold):\n",
    "        for key, value in sorted_d.items():\n",
    "            if (temp < max_threshold):\n",
    "                temp += 1\n",
    "                print(key + ': ' + value * '*')\n",
    "    else:\n",
    "        for key, value in sorted_d.items():\n",
    "            print(key + ': ' + value * '*')\n",
    "\n",
    "visualize_word_frequency(word_frequencies, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afec33",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "f1899c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied analysis: \n",
      "{'extract_information': {'emails': [], 'phone_numbers': [], 'dates': [], 'times': [], 'prices': []}, 'analyze_sentiment': 9}\n"
     ]
    }
   ],
   "source": [
    "def apply_analysis(text, operations):\n",
    "    text = preprocess_text(text)\n",
    "    result = {}\n",
    "    for i in operations:\n",
    "        result[i.__name__] = i(text)\n",
    "    return result\n",
    "\n",
    "analysis = apply_analysis(preprocessed_text, [extract_information, analyze_sentiment])\n",
    "\n",
    "print(f\"Applied analysis: \\n{analysis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b6758c",
   "metadata": {},
   "source": [
    "# Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "038a43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalyzer:\n",
    "    def __init__(self, text: str):\n",
    "        self.original_text = text\n",
    "        self.cleaned_text = self.preprocess_text(text)\n",
    "\n",
    "    def preprocess_text(self, text: str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z\\s.!?0-9\\']', '', text)\n",
    "        text = re.sub(r'([.!?])\\1+', r'\\1', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    def word_frequency(self) -> dict:\n",
    "        if (self.cleaned_text):\n",
    "            stop_words = [\"and\", \"the\", \"is\", \"in\", \"it\", \"you\", \"that\", \"to\", \"of\", \"a\", \"with\", \"for\", \"on\", \"this\", \"at\", \"by\", \"an\", \"i\"]\n",
    "            arr = re.sub(r'[^a-z\\s\\']', '', self.cleaned_text).split()\n",
    "            d = {}\n",
    "            for word in arr:\n",
    "                if (word not in dict.keys(d) and word not in stop_words):\n",
    "                    d[word] = 1\n",
    "                elif (word not in stop_words):\n",
    "                    d[word] += 1\n",
    "            return d\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    def extract_information(self, *, email_pattern=r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}',\n",
    "                        phone_pattern=r'[\\+(]\\d[\\d \\-()]{8,12}\\d', date_pattern=r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b',\n",
    "                        time_pattern=r'\\b\\d{1,2}:\\d{2}\\s*[APap][mM]?\\b', price_pattern=r'\\$\\d+(?:\\.\\d{2})?', **patterns) -> dict:\n",
    "        d = {}\n",
    "        d['emails'] = re.findall(email_pattern, self.original_text)\n",
    "        d['phone_numbers'] = re.findall(phone_pattern, self.original_text)\n",
    "        d['dates'] = re.findall(date_pattern, self.original_text)\n",
    "        d['times'] = re.findall(time_pattern, self.original_text)\n",
    "        d['prices'] = re.findall(price_pattern, self.original_text)\n",
    "        for i in patterns:\n",
    "            d['i'] = re.findall(i, self.original_text)\n",
    "        return d\n",
    "\n",
    "    def analyze_sentiment(self, positive_words = [\"good\", \"great\", \"happy\", \"joy\", \"excellent\", \"fantastic\", \"love\", \"best\", \"amazing\", \"fun\"],\n",
    "                          negative_words = [\"bad\", \"sad\", \"hate\", \"terrible\", \"awful\", \"poor\", \"worst\"]) -> int:\n",
    "        arr = re.sub(r'[^a-z\\s\\']', '', self.cleaned_text).split()\n",
    "        for i in range(0, len(arr)):\n",
    "            if (arr[i][-1] == '.'):\n",
    "                arr[i] = arr[i][:-1]\n",
    "        number_of_positive_words = 0\n",
    "        for word in positive_words:\n",
    "            number_of_positive_words += arr.count(word)\n",
    "        number_of_negative_words = 0\n",
    "        for word in negative_words:\n",
    "            number_of_negative_words += arr.count(word)\n",
    "        return number_of_positive_words - number_of_negative_words\n",
    "\n",
    "    def summarize_text(self, compression_ratio: float, min_threshold=2) -> str:\n",
    "        sentences = re.split(r'(?<=\\w[.!?])\\s+', self.cleaned_text)\n",
    "        needed_sentences = max(int(len(sentences) * compression_ratio), min_threshold)\n",
    "    \n",
    "        def summarize(sentences):\n",
    "            if len(sentences) <= needed_sentences:\n",
    "                return\n",
    "            level = 100000000000\n",
    "            len_sentence_rang = 3\n",
    "            count_words_rang = 10\n",
    "            sentiment_rang = 100\n",
    "            for sentence in sentences:\n",
    "                sentence_level = len(sentence) * len_sentence_rang + len(sentence.split(' ')) * count_words_rang \\\n",
    "                + abs(analyze_sentiment(sentence)) * sentiment_rang\n",
    "                if (sentence_level < level):\n",
    "                    level = sentence_level\n",
    "                    min_level_sentence = sentence\n",
    "            sentences.remove(min_level_sentence)\n",
    "            summarize(sentences)\n",
    "        summarize(sentences)\n",
    "        sentences = ' '.join(sentences)\n",
    "        return sentences\n",
    "\n",
    "    def visualize_word_frequency(self, max_threshold=None) -> None:\n",
    "        sorted_d = {key: value for key, value in sorted(self.word_frequency().items(), key=lambda item: item[1], reverse=True)}\n",
    "        temp = 0\n",
    "        if (max_threshold):\n",
    "            for key, value in sorted_d.items():\n",
    "                if (temp < max_threshold):\n",
    "                    temp += 1\n",
    "                    print(key + ': ' + value * '*')\n",
    "        else:\n",
    "            for key, value in sorted_d.items():\n",
    "                print(key + ': ' + value * '*')\n",
    "\n",
    "    def apply_analysis(self, analysis_functions: list) -> dict:\n",
    "        result = {}\n",
    "        for i in analysis_functions:\n",
    "            result[i.__name__] = i(self.cleaned_text)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb1fa3a",
   "metadata": {},
   "source": [
    "# Task 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "f079c3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed text:\n",
      "selling concord a task that might have been easy five years ago feels like rolling a boulder up a hill in 2024.\n",
      "developed by new studio firewalk and published by playstation studios concord is a 5v5 hero shooter that riffs on\n",
      "guardians of the galaxy with a cassetteera take on a pulpy space setting....\n",
      "\n",
      "Most frequent words:\n",
      "concord: 18 times.\n",
      "like: 12 times.\n",
      "it's: 10 times.\n",
      "but: 10 times.\n",
      "one: 10 times.\n",
      "hero: 9 times.\n",
      "more: 9 times.\n",
      "game: 8 times.\n",
      "modes: 8 times.\n",
      "be: 8 times.\n",
      "up: 7 times.\n",
      "shooter: 7 times.\n",
      "abilities: 7 times.\n",
      "\n",
      "Sentiment score:\n",
      "10\n",
      "\n",
      "Summarized text:\n",
      "pros has all the hero shooter staples you'd expect focus on skillsgunplay having some nuance creates fun learning curve gameplay modes are fun cons\n",
      "uninspiring design restrictions on gameplay modes dull enjoyment weird graphics decisions on pc familiar elements with minor twists that aren't\n",
      "earthshattering don't expect anything gamechanging from concord it's difficult to discuss concord without talking about overwatch so it's best to get\n",
      "it out of the way early. final thoughts review score 35 solid or good by screen rant's review metric the upfront cost of concord might make it a hard\n",
      "sell in 2024 but it's certainly nice to play a live service shooter that doesn't feel designed to ask for money at every turn.\n"
     ]
    }
   ],
   "source": [
    "Sic_Mundus_Creatus_Est = TextAnalyzer(wrapped)\n",
    "\n",
    "print(\"Preprocessed text:\")\n",
    "print(textwrap.fill(Sic_Mundus_Creatus_Est.cleaned_text[:300], width=120) + '...')\n",
    "print()\n",
    "word_frequency_dic = Sic_Mundus_Creatus_Est.word_frequency()\n",
    "sorted_dic = {key: value for key, value in sorted(word_frequency_dic.items(), key=lambda item: item[1], reverse=True)}\n",
    "temp = 0\n",
    "s = ''\n",
    "print(\"Most frequent words:\")\n",
    "for i in sorted_dic.keys():\n",
    "    if (temp < 13):\n",
    "        print(i + \": \" + str(sorted_dic[i]) + \" times.\")\n",
    "        temp += 1\n",
    "print()\n",
    "print(\"Sentiment score:\")\n",
    "print(Sic_Mundus_Creatus_Est.analyze_sentiment())\n",
    "print()\n",
    "print(\"Summarized text:\")\n",
    "print(textwrap.fill(Sic_Mundus_Creatus_Est.summarize_text(0.05), width=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b0182",
   "metadata": {},
   "source": [
    "By looking at the most frequently occurring words and the condensed text, we can conclude that the original text is a review of a shooter called 'Concord.' According to the sentiment analysis, the review is more positive than negative. Based on the summary, the shooter has all the core elements of games in this genre but slightly falls short in graphics. The author also notes that the game does not require much in-game spending for an enjoyable experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a67780",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "In the course of this work, I developed a text analysis tool. This tool can process various types of text entries, generating insights and statistics. It includes the following functions:\n",
    "  - Text cleaning\n",
    "  - Word frequency counting\n",
    "  - Pattern-based information extraction\n",
    "  - Sentiment analysis\n",
    "  - Text compression based on specific metrics\n",
    "  - Word frequency visualization\n",
    "  - Applying multiple functions simultaneously\n",
    "  - A dedicated class containing all the analysis functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
